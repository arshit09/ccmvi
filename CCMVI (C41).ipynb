{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d15409f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics\n",
    "import shutil\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091e8e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Imputing data in C:\\Users\\arshi\\Desktop\\Presentation\\Output\\Data_1_AW_10%.csv...\n",
      "NRMS: 0.278360811807391\n",
      "Run time: 1.4304254055023193\n",
      "Key Parameteres:\n",
      "-Complete class iteration: 800\n",
      "-Incomplete class iterations: 200\n",
      "-Threshold overhead iterations: 57\n",
      "\n",
      "Total 1 files imputed.\n"
     ]
    }
   ],
   "source": [
    "# set input path of the folder which contains multilpe CSV files to impute the data\n",
    "input_folder = r\"C:\\Users\\arshi\\Desktop\\Presentation\\Input\"\n",
    "\n",
    "# set the output path to save the all the CSV files after imputation\n",
    "output_folder = r\"C:\\Users\\arshi\\Desktop\\Presentation\\Output\"\n",
    "\n",
    "# set the path of original dataset to find NRMS values\n",
    "original_dataset = r'C:\\Users\\arshi\\Desktop\\Project DM\\BlackBoard\\Complete datasets\\Data_1.csv'\n",
    "\n",
    "# location to save nrms values (specify file name with extension - it will be created automatically)\n",
    "analysis_file = r\"C:\\Users\\arshi\\Desktop\\Presentation\\Analysis\\Analysis.csv\"\n",
    "\n",
    "# write header name in the output NRMS.csv files\n",
    "if(analysis_file and original_dataset):\n",
    "    f = open(analysis_file, \"a\")\n",
    "    f.write(\"Dataset File,NRMS,Run time,Key Parameters\" + \"\\n\" )\n",
    "    f.close()\n",
    "\n",
    "# scan all the CSV files located in the input_folder path\n",
    "all_files = []\n",
    "for path, currentDirectory, files in os.walk(input_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            all_files.append(os.path.join(path, file))\n",
    "\n",
    "# copy all the files from input_folder path to output_folder path\n",
    "for i in range(0, len(all_files)):\n",
    "    shutil.copy2(all_files[i], output_folder)\n",
    "\n",
    "# create a list of all the CSV file\n",
    "output_folder_files = []\n",
    "for dirpath,_,filenames in os.walk(output_folder):\n",
    "    for f in filenames:\n",
    "        output_folder_files.append(os.path.abspath(os.path.join(dirpath, f)))\n",
    "output_folder_files\n",
    "\n",
    "# executing the algorithm for each CSV file\n",
    "for n in range(0, len(output_folder_files)):\n",
    "    # variable to store number of iterations\n",
    "    incomplete_class_iteration = 0\n",
    "    complete_class_iteration = 0\n",
    "    threshold_overhead_iterations = 0\n",
    "    validity = 0\n",
    "    print(str(n+1) + \". Imputing data in \" + output_folder_files[n] + \"...\")\n",
    "    \n",
    "    # read excel sheet with no haeder (because the data provided has no header)\n",
    "    df = pd.read_csv (output_folder_files[n], header=None)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # find last column in the data as it has class lables\n",
    "    lastcolumn = len(df.columns)-1\n",
    "    dfclass = df.iloc[:,lastcolumn]\n",
    "    class_names = dfclass.to_numpy()\n",
    "    \n",
    "# ----------------------------------------------MODULE A starts----------------------------------------------\n",
    "    # store unique class lables\n",
    "    unique_list = []\n",
    "    for x in class_names:\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    unique_list.sort()\n",
    "\n",
    "    # divide dataset into unique classes\n",
    "    for m in range(0, len(unique_list)):\n",
    "        class_column = df.iloc[:,len(df.columns)-1];\n",
    "        oneclass = []\n",
    "    \n",
    "        # create dataset of unique class\n",
    "        for i in range(0, len(df.index)):\n",
    "            if(int(unique_list[m]) == int(class_column.iloc[i])):\n",
    "                oneclass.append(df.iloc[i,:])\n",
    "\n",
    "        df_class = pd.DataFrame(oneclass)\n",
    "        complete_class = []\n",
    "        incomplete_class = []\n",
    "        nan_count = 0\n",
    "\n",
    "        # scanning each samples\n",
    "        for i in range(0, len(df_class.index)):  \n",
    "            nan_count = 0\n",
    "            # scanning each feature\n",
    "            for j in range(0, len(df_class.columns)):\n",
    "                # if the value is NaN\n",
    "                if(math.isnan(float(df_class.iloc[i,:].iloc[j]))):\n",
    "                    nan_count += 1\n",
    "            if(nan_count > 0):\n",
    "                incomplete_class.append(df_class.iloc[i,:])\n",
    "                incomplete_class_iteration += 1\n",
    "            else:\n",
    "                complete_class.append(df_class.iloc[i,:])\n",
    "                complete_class_iteration += 1\n",
    "\n",
    "        # create dataframe for complete and incomplete dataset\n",
    "        df_complete_class = pd.DataFrame(complete_class)\n",
    "        df_incomplete_class = pd.DataFrame(incomplete_class)\n",
    "\n",
    "        class_center = []\n",
    "        class_std = []\n",
    "\n",
    "        if(len(df_incomplete_class.index) == len(df_class.index)):\n",
    "            # set validity flag if there is no data in complete class\n",
    "            validity = 1\n",
    "        else:\n",
    "            # calculate class center\n",
    "            for i in range (0, len(df_complete_class.columns)):\n",
    "                class_center.append(df_complete_class.iloc[:,i].mean())\n",
    "\n",
    "            # calculate standard deviation\n",
    "            for i in range (0, len(df_complete_class.columns) - 1):\n",
    "                class_std.append(np.std(df_complete_class.iloc[:,i]))\n",
    "\n",
    "            # calculate euclidean distance\n",
    "            distance = []\n",
    "            for i in range (0, len(df_complete_class.index)):\n",
    "                conv_df_to_array = df_complete_class.iloc[i,:len(df.columns)].to_numpy()\n",
    "                distance.append(np.linalg.norm(class_center - conv_df_to_array))\n",
    "            \n",
    "            # calculate threshold\n",
    "            threshold = np.median(distance)\n",
    "#----------------------------------------------MODULE A ends----------------------------------------------\n",
    "\n",
    "#----------------------------------------------MODULE B starts----------------------------------------------\n",
    "            \n",
    "            for i in range(0, len(df_incomplete_class.index)):\n",
    "                nan_count = 0\n",
    "                # count number of missing values in the sample\n",
    "                for j in range(0, len(df_incomplete_class.columns)):\n",
    "                    if(math.isnan(float(df_incomplete_class.iloc[i,:].iloc[j]))):\n",
    "                        nan_count += 1\n",
    "                # if sample has one missing value\n",
    "                if(nan_count == 1):\n",
    "                    for j in range(0, len(df_incomplete_class.columns)):\n",
    "                        if(math.isnan(float(df_incomplete_class.iloc[i,:].iloc[j]))):\n",
    "                            # fill it with the class center\n",
    "                            df_incomplete_class.iloc[i,:].iloc[j] = class_center[j]\n",
    "                            # find euclidean distance with the class center\n",
    "                            temp_distance = []\n",
    "                            conv_df_to_array = df_incomplete_class.iloc[i,:].to_numpy()\n",
    "                            # calculate euclidean distance\n",
    "                            temp_distance = (np.linalg.norm(class_center - conv_df_to_array))\n",
    "                            \n",
    "                            #compare distance with the threshold\n",
    "                            if(temp_distance >= threshold):\n",
    "                                threshold_overhead_iterations += 1\n",
    "                                df_incomplete_class.iloc[i,:].iloc[j] = class_center[j] - class_std[j]\n",
    "                \n",
    "                # if sample has more than one missing value\n",
    "                else:\n",
    "                    for j in range(0, len(df_incomplete_class.columns)):\n",
    "                        if(math.isnan(float(df_incomplete_class.iloc[i,:].iloc[j]))):\n",
    "                            missing = []\n",
    "                            # saving the index of the missing value of the sample\n",
    "                            missing.append(j)\n",
    "                            # fill it with the class center\n",
    "                            df_incomplete_class.iloc[i,:].iloc[j] = class_center[j]\n",
    "                            # find euclidean distance with the class center\n",
    "                            temp_distance = []\n",
    "                            conv_df_to_array = df_incomplete_class.iloc[i,:].to_numpy()\n",
    "                            # calculate euclidean distance\n",
    "                            temp_distance = np.linalg.norm(class_center - conv_df_to_array)\n",
    "                            #compare distance with the threshold\n",
    "                            if(temp_distance >= threshold):\n",
    "                                threshold_overhead_iterations += 1\n",
    "                                temp_df = pd.DataFrame()\n",
    "                                for j in range(0, len(df_incomplete_class.columns)):\n",
    "                                    for k in range(0, len(missing)):\n",
    "                                        # impute with + and - std\n",
    "                                        if(math.isnan(float(df_incomplete_class.iloc[i,:].iloc[j]))):\n",
    "                                            # storing all the possible values of + and - std\n",
    "                                            df_incomplete_class.iloc[i,:].iloc[missing[k]] = class_center[j] + class_std[j]\n",
    "                                            temp_df.append(df_incomplete_class.iloc[i,:])\n",
    "                                            df_incomplete_class.iloc[i,:].iloc[missing[k]] = class_center[j] - class_std[j]\n",
    "                                            temp_df.append(df_incomplete_class.iloc[i,:])\n",
    "                                \n",
    "                                for j in range(0, len(temp_df.index)):\n",
    "                                    distance_array = []\n",
    "                                    conv_temp_df_to_array = temp_df.iloc[i,:].to_numpy()\n",
    "                                    # find euclidean distance with the class center\n",
    "                                    distance_array.append(np.linalg.norm(class_center - conv_temp_df_to_array))\n",
    "                                    # find the minimum distance and impute it\n",
    "                                    df_incomplete_class.iloc[i,:] = temp_df[distance_array.index(min(distance_array))]\n",
    "            \n",
    "            # save the imputed dataframe\n",
    "            df.update(df_incomplete_class)\n",
    "            df.to_csv(output_folder_files[n], index = False, header = False)\n",
    "#----------------------------------------------MODULE B ends----------------------------------------------\n",
    "\n",
    "    # calculate NRMS, Execution time and Key parameters\n",
    "    if(original_dataset):\n",
    "        imputed = pd.read_csv (output_folder_files[n], header=None).to_numpy()\n",
    "        original = pd.read_csv (original_dataset, header=None).iloc[1:].astype(float).to_numpy()       \n",
    "        num_nrms = np.linalg.norm(imputed - original)\n",
    "        deno_nrms = np.linalg.norm(original)\n",
    "        nrms = (num_nrms/deno_nrms)\n",
    "        if(analysis_file and original_dataset):\n",
    "            f = open(analysis_file, \"a\")\n",
    "            execution_time = time.time() - start_time\n",
    "            string_to_write = str(output_folder_files[n]) + \",\" + str(nrms) + \",\" + str(execution_time) + \",Complete Class Iteration: \" + str(complete_class_iteration) + \". Incomplete class iterations: \" + str(incomplete_class_iteration) + \". Threshold overhead iterations: \" + str(threshold_overhead_iterations) + \"\\n\" \n",
    "            if(validity == 1):\n",
    "                string_to_write = str(output_folder_files[n]) + \",NA,\" + str(execution_time) + \",Complete Class Iteration: \" + str(complete_class_iteration) + \". Incomplete class iterations: \" + str(incomplete_class_iteration) + \". Threshold overhead iterations: \" + str(threshold_overhead_iterations) + \"\\n\" \n",
    "            f.write(string_to_write)\n",
    "            f.close()\n",
    "        \n",
    "        if (validity == 0):\n",
    "            console_output = \"NRMS: \" + str(nrms) + \"\\n\" + \"Run time: \" + str(execution_time) + \"\\nKey Parameteres:\\n-Complete class iteration: \" + str(complete_class_iteration) + \"\\n-Incomplete class iterations: \" + str(incomplete_class_iteration) + \"\\n-Threshold overhead iterations: \" + str(threshold_overhead_iterations) + \"\\n\" \n",
    "            print(console_output)\n",
    "        else:\n",
    "            console_output = \"NRMS: NA\\nRun time: \" + str(execution_time) + \"\\nKey Parameteres:\\n-Complete class iteration: \" + str(complete_class_iteration) + \"\\n-Incomplete class iterations: \" + str(incomplete_class_iteration) + \"\\n-Threshold overhead iterations: \" + str(threshold_overhead_iterations) + \"\\n\" \n",
    "            print(console_output)    \n",
    "            \n",
    "print(\"Total \" + str(len(output_folder_files)) + \" files imputed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d87d6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a3d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
